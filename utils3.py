# -*- coding: utf-8 -*-
# @Time    : 2020/7/28 11:25 AM
# @Author  : Yinghao Qin
# @Email   : y.qin@hss18.qmul.ac.uk
# @File    : utils3.py
# @Software: PyCharm
import os
import shutil

from torch.utils.data import DataLoader
from torchvideotransforms.video_transforms import Resize, Normalize, Compose
from torchvideotransforms.volume_transforms import ClipToTensor

from jesterdataset import Offline_Video_Process


#######################################################################################
# 'utils3' is used to process offline data and give processing about proposals        #
# 1. Process offline video to produce data loader                                     #
# 2. Process the original detection results to generate proposals                     #
# 3. Collect video clips according to the proposals for classification                #
#######################################################################################


def offline_video_processing(batch_size=4, channel_nb=3, frame_nb=8, frame_size=(112, 112),
                             buffer_dir="./video_expr/Offline_Buffer/", stream_length=99, shuffle=False, num_workers=4):
    """
    It is used to process the offline video data, i.e. recorded video.
    :param batch_size: the number of frames for the whole video.
    :param channel_nb: the number of channels
    :param frame_nb: for each batch, the number of frames
    :param frame_size: the size of frame in video, e.g., (120, 120)
    :param buffer_dir: it is a buffer directory storing all of the frames in the video
    :param stream_length: the total number of frames in a video
    :param shuffle: whether to shuffle the data
    :param num_workers: the number of workers
    :return: data loader, the batch in it is (batch_size, channel_nb, frame_nb, height, width), e.g., (4,3,8,112,112)
    """
    video_transform_list = [
        Resize(frame_size),
        ClipToTensor(channel_nb=channel_nb),
        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ]
    video_transform = Compose(video_transform_list)
    dataset = Offline_Video_Process(video_dir=buffer_dir, number_of_frames=frame_nb, stream_length=stream_length,
                                    video_transform=video_transform)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)
    return dataloader


def proposals_processing(proposals, intersection_threshold, amend_left_factor=0, amend_right_factor=0, border=240,
                         clr_frame_nb=32):
    """
    This function is used to process the original proposal areas, which are generated by running detector. It contains
    two stages: first, combine some proposals and generate a new 'proposals' list; second, make some amendment the
    proposal areas.
    :param proposals: it contains all the proposals generated by running the detector, indicting the potential area with
     'gesture'. it is a list, each element of which is a tuple, e.g. [(8, 16), (12, 20)]
    :param intersection_threshold: if the intersection area of two neighboring proposals is larger than the threshold,
    the two proposals mix into a new proposal. For example, (8, 16) and (12, 20) can be combined into (8, 20)
    :param amend_left_factor: for each proposal, the left factor is added to amend the smaller value in the proposal
    :param amend_right_factor: for each proposal, the right factor is added to amend the larger value in the proposal
    :param border: the max frame id, which is used to avoid exceeding the frame boarder
    :param clr_frame_nb: the frame number of the classifier, which is used to help avoid exceeding the frame boarder
    :return: proposals after processing
    """
    pointX = 0
    pointY = 1
    while pointY < len(proposals):
        if abs(proposals[pointX][1] - proposals[pointY][0]) <= intersection_threshold:
            t = (proposals[pointX][0], proposals[pointY][1])
            proposals.pop(pointX)
            proposals.pop(pointX)
            proposals.insert(pointX, t)
        else:
            pointX = pointY
            pointY += 1

    proposals_amended = []

    for i in range(len(proposals)):
        proposals_amended.append((proposals[i][0] + amend_left_factor, proposals[i][1] + amend_right_factor))

    # check whether the proposal areas will exceed the border
    if proposals_amended[0][0] < 0:
        a = proposals_amended.pop(0)
        proposals_amended.insert(0, (0, a[1]))
    elif proposals_amended[-1][0] + clr_frame_nb > border:
        a = proposals_amended.pop(-1)
        proposals_amended.insert(len(proposals_amended), (border-clr_frame_nb, a[1]))

    return proposals_amended


def proposal_data_collection(buffer_dir, proposals, frame_nb):
    """
    It is used to collect the proposed frames for classification task.
    :param buffer_dir: the directory of the video frames
    :param proposals: proposals amended. e.g. [8, 40]. here 8 refers to the starting point of the proposal area
    :param frame_nb: the frame number
    :return:
    """
    proposals_dir = os.path.join(buffer_dir, 'proposals')
    for i in range(len(proposals)):
        save_dir = os.path.join(proposals_dir, str(proposals[i][0]))
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
            for j in range(frame_nb):
                img = os.path.join(buffer_dir, str(proposals[i][0] + j) + '.jpg')
                new_img_name = str(j) + '.jpg'
                shutil.copy(img, save_dir + '/' + new_img_name)
        else:
            for j in range(frame_nb):
                img = os.path.join(buffer_dir, str(proposals[i][0] + j) + '.jpg')
                new_img_name = str(j) + '.jpg'
                shutil.copy(img, save_dir + '/' + new_img_name)

    return proposals_dir
